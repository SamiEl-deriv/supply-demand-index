from typing import Any
import pandas as pd
from pandas.errors import ParserError
import numpy as np
import os.path as path
import re
import os
import yaml
import glob

def write_yaml(filepath : str, data : Any):
    """
    Writes data to a yaml file.

    Arguments
    ---------
    filepath : str
        The filepath to write to.
    data : Any
        The data to write to the YAML file.
    """
    with open(filepath, "w") as yaml_file:
            yaml.dump(data, yaml_file)

def read_yaml(filepath):
    """
    Reads data from a yaml file.

    Arguments
    ---------
    filepath : str
        The filepath to read from.

    Returns
    -------
    yaml_data : Any
        The yaml data as a Python Object.
    """
    try:    
        with open(filepath, 'r') as yaml_file:
            return yaml.safe_load(yaml_file)
    except FileNotFoundError:
        print(f'File not found: {filepath}')

def read_csv_feeddb(df_path : str, nrows = None, source = "QAbox") -> pd.DataFrame:
    """
    Reads csv's generated by MT5.

    Arguments
    ---------
    df_path : str
        The filepath to the csv.
    nrows : int
        The number of rows to read.
        If empty, read all rows.

    Returns
    -------
    dfs_feeddb : pandas.DataFrame
        A list of Dataframes.
    """
    try:
        df = pd.read_csv(df_path, nrows=nrows)
    except ParserError:
        df = pd.read_csv(df_path, nrows=nrows, engine='python')
    
    ns = '.%f'
    # Parse, format and handle dates
    if source == "Metabase":
        df['ts'] = pd.to_datetime(df['ts'], format='mixed')
    elif source == "QAbox":
        df = df.drop(columns=['Unnamed: 0'])
        df['ts'] = pd.to_datetime(df['ts'], format='mixed')
    else:
        raise ValueError("Invalid source. Try one of ['Metabase', 'QAbox']")
    df.sort_values('ts', inplace=True)
    return df

def read_csv_MT5(df_path : str, nrows = None) -> pd.DataFrame:
    """
    Reads csv's generated by MT5.

    Arguments
    ---------
    df_path : str
        The filepath to the csv.
    nrows : int
        The number of rows to read.
        If empty, read all rows.

    Returns
    -------
    dfs_MT5 : pandas.DataFrame
        A list of Dataframes.
    """
    DSI_raw = pd.read_csv(df_path, nrows = nrows, sep='\t', usecols=['<DATE>', '<TIME>', '<BID>', '<ASK>','<FLAGS>'])
    DSI_raw['ts'] = pd.to_datetime(DSI_raw['<DATE>'] + ' ' + DSI_raw['<TIME>'])
    DSI_raw.drop(columns=['<DATE>', '<TIME>'], inplace=True)
    DSI_raw.rename(columns=dict(zip(['<BID>', '<ASK>','<FLAGS>'], 
                                    ['bid', 'ask', 'flags'])), inplace=True)
    
    to_float = ['bid', 'ask']
    to_int = ['flags']
    DSI_raw[to_float] = DSI_raw[to_float].astype(np.float64)
    DSI_raw[to_int] = DSI_raw[to_int].astype(np.int64)

    cols = DSI_raw.columns.tolist()
    cols = cols[-1:] + cols[:-1]
    DSI_raw = DSI_raw[cols]
    return DSI_raw

def write_list_dfs_to_parquet(dfs : list[pd.DataFrame], df_path, reset=False):
    """
    Writes/appends a list of Dataframes to a parquet file.

    Arguments
    ---------
    dfs : list[pandas.DataFrame]
        The list of Dataframes to write to the csv.
    df_path : str
        The filepath to the parquet file.
    reset : bool
        If True, overwrite file.
        Else append to end of file.
    """
    for df in dfs:
        temp = df.reset_index()
        if not path.isfile(df_path) or reset:
            temp.to_parquet(df_path, engine='pyarrow')
            reset = False
        else:
            temp.to_parquet(df_path, engine='pyarrow', append=True)

def read_list_dfs_from_parquet(df_path):
    """
    Reads a parquet file containing consecutive Dataframes.

    Arguments
    ---------
    df_path : str
        The filepath to the csv.

    Returns
    -------
    dfs_parquet : list[pandas.DataFrame]
        A list of Dataframes.
    """
    df = pd.read_parquet(df_path, engine='pyarrow')
    zeroes_indices = np.append(df['index'][df['index'] == 0].index.values, df.index.values[-1] + 1)
    df = df.drop(columns='index')
    list_dfs = [df.iloc[zeroes_indices[i]:zeroes_indices[i+1],:].reset_index(drop=True) for i in range(len(zeroes_indices) - 1)]
    return list_dfs

def name_betterer_single_cB(string : str, name_changing_dict : dict[str,str]) -> str:
    """
    Changes a cBroker feed file name to be more organized.
    
    Parameters
    ----------
    string : str
        String to be modified.
    name_changing_dict : dict[str,str]
        TODO: Can't remember what happens here.
    
    Returns
    -------
    cB_cleaned : str
        Cleaned cB feed name string.
    """
    a = re.findall(r'(?<=_)[^_]+(?=_)', string)
    a[2] = a[2].replace('-','')
    a[1] = name_changing_dict[a[1]]
    return 'CTrader_'+'_'.join(a[1:3])

def name_betterer_all_cB(dir_path : str, name_changing_dict : dict[str,str]):
    """
    Convert cBroker feed names to be more organized

    Parameters
    ----------
    dir_path : str
        Directory containing files to rename.
    name_changing_dict : dict[str,str]
        TODO: Can't remember what happens here.
    """
    for root, _, files in os.walk(dir_path):
        for i in files:
            if len(i.split('_')) == 3 or i.split('_') != 'cB': continue
            os.rename(os.path.join(root, i), os.path.join(root, name_betterer_single_cB(i, name_changing_dict)+ ".csv"))

def file_filter(paths: str | list | tuple, ext: str = 'csv') -> list[str]:
    """Returns file paths with a specified extension from given paths.

    Recursively searches through directories and filters files by extension.
    Single files are included if they match the extension.

    Args:
        paths: Single path string or list/tuple of paths to search
        ext: File extension to filter for (without the dot)

    Returns:
        List of file paths matching the specified extension

    Raises:
        FileNotFoundError: If any provided path doesn't exist
        ValueError: If paths argument is not a string, list, or tuple
    """
    def _process_path(path: str) -> list[str]:
        if not os.path.exists(path):
            raise FileNotFoundError(f"Path does not exist: {path}")
        
        if os.path.isdir(path):
            return glob.glob(os.path.join(path, f"**/*.{ext}"), recursive=True)
        return [path] if path.endswith(f".{ext}") else []

    # Convert single path to list for uniform processing
    path_list = [paths] if isinstance(paths, str) else paths
    
    if not isinstance(path_list, (list, tuple)):
        raise ValueError("paths must be a string, list, or tuple")

    # Process all paths and combine results
    return [fp for path in path_list for fp in _process_path(path)]

month_word_to_number_dict = dict(zip(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], [f'{i:02d}' for i in range(1,13)]))

def BBG_csv_renamer(paths : str | list, asset : str):
    '''
    Rename BBG BGN format csvs into an orderly name: asset_YYYYMMDD.csv.
    Accepts a list or single dir/file paths.

    Parameters
    ----------
    paths : str | list
        Files or directories of files to traverse and rename.
    asset : str
        The name of the underlying asset to assign to the feed.
    '''
    file_paths = file_filter(paths)
    print(file_paths)
    for path in file_paths:
        root = '/'.join(path.split('/')[:-1])
        
        file = path.split('/')[-1]
        
        if re.findall('^\d{2}-[a-zA-Z]{3}-\d{2}.fullfeed\.csv$', file):
            file_nums = file
            for key, value in month_word_to_number_dict.items():
                file_nums = file_nums.replace(key, value)
            print(file_nums)
            file_components = file_nums.split('-')[:3]
            file_components.reverse()
            print(file_components)
            new_file_name = f'{asset}_20{"".join(file_components)}.csv'
            os.rename(os.path.join(root, file), os.path.join(root, new_file_name))

def better_describe(df : pd.DataFrame, scale_factor : float = 1):
    """
    Extended pandas describe function.
    
    Parameters
    ----------
    df : pandas.DataFrame
        DataFrame to describe.
    scale_factor : float
        Optional factor to scale mean and std by. Defaults to 1.
    
    Returns
    -------
    describe_result : pd.DataFrame
        Statistical description of `df`, including:
        * count
        * mean
        * std
        * skew
        * kurt
        * min
        * 10% quantile
        * 25% quantile
        * 50% quantile
        * 75% quantile
        * 90% quantile
        * max
    """
    stats = df.describe()
    if scale_factor:
        stats.loc['mean'] = stats.loc['mean'] * scale_factor
        stats.loc['std'] = stats.loc['std'] * np.sqrt(scale_factor)
    stats.loc['skew'] = df.skew()
    stats.loc['kurt'] = df.kurtosis()
    stats.loc['10%'] = df.quantile(0.1)
    stats.loc['90%'] = df.quantile(0.9)

    cols = ['count', 'mean', 'std', 'skew', 'kurt', # Moment stats
            'min', '10%', '25%', '50%', '75%', '90%', 'max' # Positional & quantile stats
            ]
    return stats[cols].apply(lambda x: format(x, 'f'))
